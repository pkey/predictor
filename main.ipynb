{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for reading data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTestDatasetFromCsv():\n",
    "    tweets = pd.read_csv(\"data/Test.csv\", delimiter=',', encoding='utf8')\n",
    "    return tweets\n",
    "\n",
    "def getTweetsFromCsv(filename=\"data/Tweets.csv\"):\n",
    "    tweets = pd.read_csv(filename, delimiter='\\t', names=['id', 'text', 'time'], encoding='utf8')\n",
    "    tweets['time'] = pd.to_datetime(tweets['time'])\n",
    "    tweets = tweets.sort_values('time')\n",
    "    return tweets\n",
    "\n",
    "def getPricesFromCsv(pricefile = 'data/Prices.csv'):  \n",
    "    return pd.read_csv(pricefile)\n",
    "\n",
    "def matchPriceLatency(rawPrices, delay):\n",
    "    #Get target price, applying delay\n",
    "    rawPrices['target'] = rawPrices.loc[delay:, ['open']].reset_index(drop=True)\n",
    "    \n",
    "    #Get increased by substracting difference and applying boolean expression\n",
    "    rawPrices['increased'] = rawPrices.loc[delay:, ['open']].reset_index(drop=True) - rawPrices.loc[:,['open']].reset_index(drop=True)\n",
    "    rawPrices['increased'] = rawPrices['increased'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    return rawPrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Utility functions for preprocessing and building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordSet(tweets):\n",
    "    #Get distinct set of words from messages\n",
    "    return set([i for i in ' '.join(tweets).split(' ')])\n",
    "\n",
    "def mapWordsToIntegers(words):\n",
    "    #Create word to integer map\n",
    "    return {word:index+1 for index,word in enumerate(words)}\n",
    "\n",
    "def getCleanedTweets(tweets):\n",
    "    #Removed punctuations and excess spaces\n",
    "    cleanedTweets = []\n",
    "    for tweet in tweets:\n",
    "        cleanedTweets.append(''.join(\n",
    "            [c for c in ' '.join(filter(None, tweet.split(' '))) if c not in punctuation]\n",
    "        ))\n",
    "    return cleanedTweets\n",
    "\n",
    "def preprocessTweets(tweets, sequenceLength, tweets_ints):\n",
    "    proccessedTweets = np.zeros((len(tweets), sequenceLength),dtype=int)\n",
    "    for index, tweet in enumerate(proccessedTweets):\n",
    "        tweet_int = tweets_ints[index]\n",
    "        tweet[sequenceLength - len(tweet_int):] = tweet_int\n",
    "    return proccessedTweets\n",
    "\n",
    "\n",
    "def getPriceDictionary(prices):\n",
    "    #Create dictionary of price date and whether it has increased or not\n",
    "    return {row['start']:row['increased'] for index, row in prices.iterrows()}\n",
    "\n",
    "def getTrainingData(tweets, price_map, delay=1):\n",
    "    time_keys = sorted(list(price_map.keys()))[:-delay]\n",
    "    tweets_train = tweets.loc[tweets['time'].astype(str).isin(time_keys)]\n",
    "    labels_train = [price_map[str(row['time'] + pd.Timedelta(minutes=delay))] for index,row in tweets_train.iterrows()]\n",
    "    return tweets_train, labels_train\n",
    "\n",
    "\n",
    "def getLongestTweetLength(tweets):\n",
    "    longest = 0\n",
    "    for tweet in tweets:\n",
    "        length = len(tweet.split(' '))\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "    return longest\n",
    "\n",
    "def mapTweetsToWordIntegerMaps(tweets, wordIntegers):\n",
    "    #map tweets messages to word integeres\n",
    "    proccessedTweets = []\n",
    "    for tweet in tweets:\n",
    "        proccessedTweets.append([wordIntegers[word] for word in tweet.split(' ')\n",
    "                                          if word != '' and word!= ' '])\n",
    "    return proccessedTweets \n",
    "    \n",
    "def splitData(data, labels, ratio):    \n",
    "    #Split data\n",
    "    pivotIndex = int(len(data)*ratio)\n",
    "    train_x, val_x = data[:pivotIndex], data[pivotIndex:]\n",
    "    train_y, val_y = labels[:pivotIndex], labels[pivotIndex:]\n",
    "\n",
    "    testPivotIndex = int(len(val_x)*0.5)\n",
    "    val_x, test_x = val_x[:testPivotIndex], val_x[testPivotIndex:]\n",
    "    val_y, test_y = val_y[:testPivotIndex], val_y[testPivotIndex:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(labels, predictions):\n",
    "    with tf.name_scope('accuracy'):        \n",
    "        correct_pred = tf.equal(labels,tf.cast(tf.round(tf.sigmoid(predictions)), tf.int32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "#         accuracy = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def getCost(labels, predictions):\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    return cost\n",
    "\n",
    "def getPredictions(embeding, lstm_size, lstm_layers,keep_prob, num_outputs=1):\n",
    "    outputs = buildRnn(embeding, lstm_size, lstm_layers, keep_prob)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "        num_outputs=num_outputs,activation_fn=None)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    return predictions\n",
    "\n",
    "def getBatches(x, y, batch_size=100):    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        \n",
    "def getLabels(output_dtype, batch_size):\n",
    "    return tf.placeholder(output_dtype, shape=[batch_size, 1], name='labels')\n",
    "\n",
    "def getInputs(output_dtype, batch_size, seq_len):\n",
    "    return tf.placeholder(dtype=output_dtype, shape=[batch_size, seq_len ], name=\"inputs\")\n",
    "\n",
    "def getSequenceLength():\n",
    "    return tf.placeholder(dtype=tf.float32,name='seq_len')\n",
    "\n",
    "def getLearningRate():\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        learning_rate = tf.placeholder(dtype=tf.float32, shape=(None), name=\"learning_rate\")\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "def getDropoutKeepProbability():\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def getEmbedding(inputs, word_number, embedding_size):\n",
    "    with tf.name_scope('embeding'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([word_number, embedding_size], -1, 1, seed=123))\n",
    "        embedded_words = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    return embedded_words\n",
    "\n",
    "def getRnnCell(lstm_size, keep_prob):\n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_size)\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop\n",
    "\n",
    "def buildRnn(inputs, lstm_size, lstm_layers, keep_prob):\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [getRnnCell(lstm_size, keep_prob) for _ in range(lstm_layers)])\n",
    "    rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, dtype=tf.float32)\n",
    "    return rnn\n",
    "\n",
    "def getOptimiser(lr, cost):            \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    return optimizer\n",
    "\n",
    "def formatSummaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTestDataset():\n",
    "    tweets = getTestDatasetFromCsv()\n",
    "    prices = tweets.airline_sentiment.apply(lambda x: 0 if x == 'negative' else 1)\n",
    "    \n",
    "    prices = np.array(prices).reshape(len(prices),1)\n",
    "    print(\"Tweets dataset size: {0}\".format(len(tweets)))\n",
    "    \n",
    "    word_list = getWordSet(tweets['text'])\n",
    "    word_2_int = mapWordsToIntegers(word_list)\n",
    "    tweets_ints = mapTweetsToWordIntegerMaps(tweets['text'], word_2_int)\n",
    "    \n",
    "    longest = getLongestTweetLength(tweets['text'])\n",
    "    print(\"Total unique word count: {0}\".format(len(word_list)))\n",
    "    \n",
    "    tweet_final = preprocessTweets(sequenceLength=longest, tweets=tweets_ints, tweets_ints=tweets_ints)\n",
    "    \n",
    "    n_words = len(word_2_int) + 1 \n",
    "    \n",
    "    return tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words,longest,prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for loading actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadActualDataset(latency, seq_len):\n",
    "    tweets = getTweetsFromCsv()\n",
    "    prices_raw = getPricesFromCsv() \n",
    "    \n",
    "    prices_raw = matchPriceLatency(prices_raw, latency)  \n",
    "    \n",
    "    tweets['time'] = tweets['time'].apply(lambda x: x.replace(second=0))\n",
    "    price_map = getPriceDictionary(prices_raw)\n",
    "    \n",
    "    tweets1, labels1 = getTrainingData(tweets, price_map, delay=1)\n",
    "    \n",
    "    prices = np.array(labels1).reshape(len(labels1),1)\n",
    "    print(\"Tweets dataset size: {0}\".format(len(tweets1)))\n",
    "    \n",
    "    tweets1['text2'] = tweets1.text.apply(lambda x: \" \".join(x.split(\" \")[-seq_len:]))\n",
    "    \n",
    "    cleaned_tweets = getCleanedTweets(tweets1['text2'])\n",
    "    word_list = getWordSet(cleaned_tweets)\n",
    "    word_2_int = mapWordsToIntegers(word_list)\n",
    "    tweets_ints = mapTweetsToWordIntegerMaps(cleaned_tweets, word_2_int)    \n",
    "    \n",
    "    longest = getLongestTweetLength(cleaned_tweets)\n",
    "    print(\"Total unique word count: {0}\".format(len(word_list)))\n",
    "    \n",
    "    tweet_final = preprocessTweets(sequenceLength=longest, tweets=tweets_ints, tweets_ints=tweets_ints)\n",
    "    \n",
    "    n_words = len(word_2_int) + 1 \n",
    "    \n",
    "    return tweets1, tweet_final, word_list, word_2_int, tweets_ints, n_words,longest,prices    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method for splitting dataset into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSplitDatasets(tweet_final, prices, is_logits = False):     \n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = splitData(tweet_final, prices, 0.8)    \n",
    "    if is_logits:\n",
    "        train_y = list(map(lambda x: [0,1] if x == 0 else [1,0], train_y))\n",
    "        val_y = list(map(lambda x: [0,1] if x == 0 else [1,0], val_y))\n",
    "        test_y = list(map(lambda x: [0,1] if x == 0 else [1,0], test_y))\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method for plotting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotGraph(tweets1):\n",
    "    abc = tweets1.text.apply(lambda x: len(x.split(\" \")))\n",
    "    plot = abc.sort_values().reset_index(drop=True).plot(kind='line',)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ylabel(\"item length\",size=14)\n",
    "    plt.xlabel(\"tweet number\", size=14)\n",
    "    plt.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRun = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment and model number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experinment number\n",
    "experimentNumber = 2\n",
    "\n",
    "#Model number\n",
    "modelNumber = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if testRun:\n",
    "    RnnSize = 128\n",
    "    RnnLayers = 1\n",
    "    batchSize = 64\n",
    "    sequenceLength = 36\n",
    "    embeddingSize = 32\n",
    "    learningRate = 0.001\n",
    "    keepProbability = 0.7\n",
    "    epochNumber = 20\n",
    "else:\n",
    "    RnnSize = 512\n",
    "    RnnLayers = 3\n",
    "    batchSize = 250\n",
    "    sequenceLength = 33\n",
    "    embeddingSize = 32\n",
    "    learningRate = 0.001\n",
    "    keepProbability = 0.7\n",
    "    epochNumber = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets dataset size: 725356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique word count: 385526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n('Train set: \\t\\t(580284, 33)', '\\nValidation set: \\t(72536, 33)', '\\nTest set: \\t\\t(72536, 33)')\n"
     ]
    }
   ],
   "source": [
    "if testRun:\n",
    "    tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words, longest, prices = loadTestDataset()\n",
    "else: \n",
    "    tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words, longest,prices = loadActualDataset(\n",
    "    1, sequenceLength)\n",
    "    #plotGraph(tweets)\n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = getSplitDatasets(tweet_final, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580284, 33)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'cost/mean_squared_error/value:0' shape=() dtype=float32>, <tf.Tensor 'predictions/fully_connected/BiasAdd:0' shape=(250, 1) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = getInputs(tf.int32, batchSize, sequenceLength)\n",
    "    labels = getLabels(tf.int32,batchSize)\n",
    "    lr = getLearningRate()\n",
    "    drop_keep_porob = getDropoutKeepProbability()\n",
    "    embed = getEmbedding(inputs=inputs, word_number=n_words, embedding_size=embeddingSize)\n",
    "    \n",
    "    predictions = getPredictions(embed, RnnSize, RnnLayers, drop_keep_porob)\n",
    "    cost = getCost(labels, predictions)\n",
    "    accuracy = getAccuracy(labels, predictions)\n",
    "       \n",
    "    print(cost,predictions)\n",
    "    optimiser = getOptimiser(lr, cost)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    for var in tf.trainable_variables('fully_connected'):\n",
    "        name = 'fc-{0}'\n",
    "        if 'weights' in var.name:\n",
    "            name = name.format('weights')\n",
    "        else:\n",
    "            name = name.format('biases')\n",
    "        with tf.variable_scope(name):\n",
    "            formatSummaries(var)  \n",
    "    \n",
    "    for var in tf.trainable_variables('dense'):\n",
    "        print(var.name)\n",
    "        name = var.name.split(\"/\")[0]+\"-{0}\"\n",
    "        if 'kernel' in var.name:\n",
    "            name = name.format('weights')\n",
    "        else:\n",
    "            name = name.format('biases')\n",
    "        with tf.variable_scope(name):\n",
    "            formatSummaries(var)  \n",
    "\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 5', 'Train loss: 0.26730105')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 10', 'Train loss: 0.24786839')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 15', 'Train loss: 0.26019669')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 20', 'Train loss: 0.27580079')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 25', 'Train loss: 0.34650776')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.40016553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 30', 'Train loss: 0.20394731')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 35', 'Train loss: 0.24118055')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 40', 'Train loss: 0.26576230')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 45', 'Train loss: 0.27044576')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 50', 'Train loss: 0.27084273')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a1485890a0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                                 \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                 drop_keep_porob:1.0}\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                         \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val acc: {:.8f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paulkutka/anaconda3/envs/predictEnv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paulkutka/anaconda3/envs/predictEnv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paulkutka/anaconda3/envs/predictEnv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paulkutka/anaconda3/envs/predictEnv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paulkutka/anaconda3/envs/predictEnv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        train_writer = tf.summary.FileWriter(\"./tensorboard-logs/try{0}/train\".format(experimentNumber),\n",
    "                                      sess.graph)\n",
    "        validation_writer = tf.summary.FileWriter(\"./tensorboard-logs/try{0}/val\".format(experimentNumber),\n",
    "                              sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "        for epoch in range(epochNumber):\n",
    "            for index, (x, y) in enumerate(getBatches(train_x, train_y, batchSize),1):\n",
    "                feed = {\n",
    "                        inputs: x, \n",
    "                        labels:y, \n",
    "                        lr: learningRate,\n",
    "                        drop_keep_porob: keepProbability}\n",
    "                \n",
    "                sess.run([optimiser], feed_dict=feed)\n",
    "\n",
    "                if iteration%5==0:\n",
    "                    summary,loss = sess.run([merged, cost],\n",
    "                        feed_dict=feed)                    \n",
    "                    print(\n",
    "                    \"Epoch: {0}/{1}\".format(epoch, epochNumber),\n",
    "                    \"Iteration: {0}\".format(iteration),\n",
    "                    \"Train loss: {:.8f}\".format(loss))\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                if iteration%25==0:\n",
    "                    val_acc = []\n",
    "                    for x1,y1 in getBatches(val_x, val_y, batchSize):\n",
    "                        feed = {inputs: x1, \n",
    "                                labels: y1,\n",
    "                                lr: learningRate,\n",
    "                                drop_keep_porob:1.0}\n",
    "                        summary,batch_acc = sess.run([merged, accuracy], feed_dict=feed)\n",
    "                        val_acc.append(batch_acc)\n",
    "                    print(\"Val acc: {:.8f}\".format(np.mean(val_acc)))\n",
    "                    validation_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration +=1\n",
    "            saver.save(sess, \"checkpoints/model{0}/predictor.ckpt\".format(modelNumber))\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model1/predictor.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.712\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/model{0}'.format(modelNumber)))\n",
    "    for ii, (x, y) in enumerate(getBatches(test_x, test_y, batchSize), 1):\n",
    "        feed = {inputs: x,\n",
    "                labels: y,\n",
    "                drop_keep_porob: 1.}\n",
    "        batch_acc = sess.run([accuracy], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(1-np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
