{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-635ead929f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for reading data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestDatasetFromCsv():\n",
    "    tweets = pd.read_csv(\"data/Test.csv\", delimiter=',', encoding='utf8')\n",
    "    return tweets\n",
    "\n",
    "def getTweetsFromCsv(filename=\"data/Tweets.csv\"):\n",
    "    tweets = pd.read_csv(filename, delimiter='\\t', names=['id', 'text', 'time'], encoding='utf8')\n",
    "    tweets['time'] = pd.to_datetime(tweets['time'])\n",
    "    tweets = tweets.sort_values('time')\n",
    "    return tweets\n",
    "\n",
    "def getPricesFromCsv(pricefile = 'data/Prices.csv'):  \n",
    "    return pd.read_csv(pricefile)\n",
    "\n",
    "def matchPriceDelay(rawPrices, delay):\n",
    "    #Get target price, applying delay\n",
    "    rawPrices['target'] = rawPrices.loc[delay:, ['open']].reset_index(drop=True)\n",
    "    \n",
    "    #Get increased by substracting difference and applying boolean expression\n",
    "    rawPrices['increased'] = rawPrices.loc[delay:, ['open']].reset_index(drop=True) - rawPrices.loc[:,['open']].reset_index(drop=True)\n",
    "    rawPrices['increased'] = rawPrices['increased'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    return rawPrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Utility functions for preprocessing and building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordSet(tweets):\n",
    "    #Get distinct set of words from messages\n",
    "    return set([i for i in ' '.join(tweets).split(' ')])\n",
    "\n",
    "def mapWordsToIntegers(words):\n",
    "    #Create word to integer map\n",
    "    return {word:index+1 for index,word in enumerate(words)}\n",
    "\n",
    "def getCleanedTweets(tweets):\n",
    "    #Removed punctuations and excess spaces\n",
    "    cleanedTweets = []\n",
    "    for tweet in tweets:\n",
    "        cleanedTweets.append(''.join(\n",
    "            [c for c in ' '.join(filter(None, tweet.split(' '))) if c not in punctuation]\n",
    "        ))\n",
    "    return cleanedTweets\n",
    "\n",
    "def preprocessTweets(tweets, sequenceLength, tweets_ints):\n",
    "    proccessedTweets = np.zeros((len(tweets), sequenceLength),dtype=int)\n",
    "    for index, tweet in enumerate(proccessedTweets):\n",
    "        tweet_int = tweets_ints[index]\n",
    "        tweet[sequenceLength - len(tweet_int):] = tweet_int\n",
    "    return proccessedTweets\n",
    "\n",
    "\n",
    "def getPriceDictionary(prices):\n",
    "    #Create dictionary of price date and whether it has increased or not\n",
    "    return {row['start']:row['increased'] for index, row in prices.iterrows()}\n",
    "\n",
    "def getTrainingData(tweets, price_map, delay=1):\n",
    "    time_keys = sorted(list(price_map.keys()))[:-delay]\n",
    "    tweets_train = tweets.loc[tweets['time'].astype(str).isin(time_keys)]\n",
    "    labels_train = [price_map[str(row['time'] + pd.Timedelta(minutes=delay))] for index,row in tweets_train.iterrows()]\n",
    "    return tweets_train, labels_train\n",
    "\n",
    "\n",
    "def getLongestTweetLength(tweets):\n",
    "    longest = 0\n",
    "    for tweet in tweets:\n",
    "        length = len(tweet.split(' '))\n",
    "        if length > longest:\n",
    "            longest = length\n",
    "    return longest\n",
    "\n",
    "def mapTweetsToWordIntegerMaps(tweets, wordIntegers):\n",
    "    #map tweets messages to word integeres\n",
    "    proccessedTweets = []\n",
    "    for tweet in tweets:\n",
    "        proccessedTweets.append([wordIntegers[word] for word in tweet.split(' ')\n",
    "                                          if word != '' and word!= ' '])\n",
    "    return proccessedTweets \n",
    "    \n",
    "def splitData(data, labels, ratio):    \n",
    "    #Split data\n",
    "    pivotIndex = int(len(data)*ratio)\n",
    "    train_x, val_x = data[:pivotIndex], data[pivotIndex:]\n",
    "    train_y, val_y = labels[:pivotIndex], labels[pivotIndex:]\n",
    "\n",
    "    testPivotIndex = int(len(val_x)*0.5)\n",
    "    val_x, test_x = val_x[:testPivotIndex], val_x[testPivotIndex:]\n",
    "    val_y, test_y = val_y[:testPivotIndex], val_y[testPivotIndex:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(labels, predictions):\n",
    "    with tf.name_scope('accuracy'):        \n",
    "        correct_pred = tf.equal(labels,tf.cast(tf.round(tf.sigmoid(predictions)), tf.int32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "#         accuracy = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def getCost(labels, predictions):\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    return cost\n",
    "\n",
    "def getPredictions(embeding, lstm_size, lstm_layers,keep_prob, num_outputs=1):\n",
    "    outputs = buildRnn(embeding, lstm_size, lstm_layers, keep_prob)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "        num_outputs=num_outputs,activation_fn=None)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    return predictions\n",
    "\n",
    "def getBatches(x, y, batch_size=100):    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        \n",
    "def getLabels(output_dtype, batch_size):\n",
    "    return tf.placeholder(output_dtype, shape=[batch_size, 1], name='labels')\n",
    "\n",
    "def getInputs(output_dtype, batch_size, seq_len):\n",
    "    return tf.placeholder(dtype=output_dtype, shape=[batch_size, seq_len ], name=\"inputs\")\n",
    "\n",
    "def getSequenceLength():\n",
    "    return tf.placeholder(dtype=tf.float32,name='seq_len')\n",
    "\n",
    "def getLearningRate():\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        learning_rate = tf.placeholder(dtype=tf.float32, shape=(None), name=\"learning_rate\")\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "    return learning_rate\n",
    "\n",
    "def getDropoutKeepProbability():\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def getEmbedding(inputs, word_number, embedding_size):\n",
    "    with tf.name_scope('embeding'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([word_number, embedding_size], -1, 1, seed=123))\n",
    "        embedded_words = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    return embedded_words\n",
    "\n",
    "def getRnnCell(lstm_size, keep_prob):\n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(num_units=lstm_size)\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop\n",
    "\n",
    "def buildRnn(inputs, lstm_size, lstm_layers, keep_prob):\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [getRnnCell(lstm_size, keep_prob) for _ in range(lstm_layers)])\n",
    "    rnn, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, dtype=tf.float32)\n",
    "    return rnn\n",
    "\n",
    "def getOptimiser(lr, cost):            \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    return optimizer\n",
    "\n",
    "def formatSummaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestDataset():\n",
    "    tweets = getTestDatasetFromCsv()\n",
    "    prices = tweets.airline_sentiment.apply(lambda x: 0 if x == 'negative' else 1)\n",
    "    \n",
    "    prices = np.array(prices).reshape(len(prices),1)\n",
    "    print(\"Tweets dataset size: {0}\".format(len(tweets)))\n",
    "    \n",
    "    word_list = getWordSet(tweets['text'])\n",
    "    word_2_int = mapWordsToIntegers(word_list)\n",
    "    tweets_ints = mapTweetsToWordIntegerMaps(tweets['text'], word_2_int)\n",
    "    \n",
    "    longest = getLongestTweetLength(tweets['text'])\n",
    "    print(\"Total unique word count: {0}\".format(len(word_list)))\n",
    "    \n",
    "    tweet_final = preprocessTweets(sequenceLength=longest, tweets=tweets_ints, tweets_ints=tweets_ints)\n",
    "    \n",
    "    n_words = len(word_2_int) + 1 \n",
    "    \n",
    "    return tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words,longest,prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method for loading actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadActualDataset(latency, seq_len):\n",
    "    tweets = getTweetsFromCsv()\n",
    "    prices_raw = getPricesFromCsv() \n",
    "    \n",
    "    prices_raw = matchPriceDelay(prices_raw, latency)  \n",
    "    \n",
    "    tweets['time'] = tweets['time'].apply(lambda x: x.replace(second=0))\n",
    "    price_map = getPriceDictionary(prices_raw)\n",
    "    \n",
    "    tweets1, labels1 = getTrainingData(tweets, price_map, delay=1)\n",
    "    \n",
    "    prices = np.array(labels1).reshape(len(labels1),1)\n",
    "    print(\"Tweets dataset size: {0}\".format(len(tweets1)))\n",
    "    \n",
    "    tweets1['text2'] = tweets1.text.apply(lambda x: \" \".join(x.split(\" \")[-seq_len:]))\n",
    "    \n",
    "    cleaned_tweets = getCleanedTweets(tweets1['text2'])\n",
    "    word_list = getWordSet(cleaned_tweets)\n",
    "    word_2_int = mapWordsToIntegers(word_list)\n",
    "    tweets_ints = mapTweetsToWordIntegerMaps(cleaned_tweets, word_2_int)    \n",
    "    \n",
    "    longest = getLongestTweetLength(cleaned_tweets)\n",
    "    print(\"Total unique word count: {0}\".format(len(word_list)))\n",
    "    \n",
    "    tweet_final = preprocessTweets(sequenceLength=longest, tweets=tweets_ints, tweets_ints=tweets_ints)\n",
    "    \n",
    "    n_words = len(word_2_int) + 1 \n",
    "    \n",
    "    return tweets1, tweet_final, word_list, word_2_int, tweets_ints, n_words,longest,prices    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method for splitting dataset into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSplitDatasets(tweet_final, prices, is_logits = False):     \n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = splitData(tweet_final, prices, 0.8)    \n",
    "    if is_logits:\n",
    "        train_y = list(map(lambda x: [0,1] if x == 0 else [1,0], train_y))\n",
    "        val_y = list(map(lambda x: [0,1] if x == 0 else [1,0], val_y))\n",
    "        test_y = list(map(lambda x: [0,1] if x == 0 else [1,0], test_y))\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Method for plotting graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(tweets1):\n",
    "    abc = tweets1.text.apply(lambda x: len(x.split(\" \")))\n",
    "    plot = abc.sort_values().reset_index(drop=True).plot(kind='line',)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ylabel(\"item length\",size=14)\n",
    "    plt.xlabel(\"tweet number\", size=14)\n",
    "    plt.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRun = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment and model number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experinment number\n",
    "experimentNumber = 2\n",
    "\n",
    "#Model number\n",
    "modelNumber = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testRun:\n",
    "    RnnSize = 128\n",
    "    RnnLayers = 1\n",
    "    batchSize = 64\n",
    "    sequenceLength = 36\n",
    "    embeddingSize = 32\n",
    "    learningRate = 0.001\n",
    "    keepProbability = 0.7\n",
    "    epochNumber = 20\n",
    "else:\n",
    "    RnnSize = 512\n",
    "    RnnLayers = 3\n",
    "    batchSize = 250\n",
    "    sequenceLength = 33\n",
    "    embeddingSize = 32\n",
    "    learningRate = 0.001\n",
    "    keepProbability = 0.7\n",
    "    epochNumber = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets dataset size: 1024916\n",
      "Total unique word count: 383670\n"
     ]
    }
   ],
   "source": [
    "if testRun:\n",
    "    tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words, longest, prices = loadTestDataset()\n",
    "else: \n",
    "    tweets, tweet_final, word_list, word_2_int, tweets_ints, n_words, longest,prices = loadActualDataset(\n",
    "    1, sequenceLength)\n",
    "    #plotGraph(tweets)\n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = getSplitDatasets(tweet_final, prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(819932, 33)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'cost/mean_squared_error/value:0' shape=() dtype=float32>, <tf.Tensor 'predictions/fully_connected/BiasAdd:0' shape=(250, 1) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = getInputs(tf.int32, batchSize, sequenceLength)\n",
    "    labels = getLabels(tf.int32,batchSize)\n",
    "    lr = getLearningRate()\n",
    "    drop_keep_porob = getDropoutKeepProbability()\n",
    "    embed = getEmbedding(inputs=inputs, word_number=n_words, embedding_size=embeddingSize)\n",
    "    \n",
    "    predictions = getPredictions(embed, RnnSize, RnnLayers, drop_keep_porob)\n",
    "    cost = getCost(labels, predictions)\n",
    "    accuracy = getAccuracy(labels, predictions)\n",
    "       \n",
    "    print(cost,predictions)\n",
    "    optimiser = getOptimiser(lr, cost)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    for var in tf.trainable_variables('fully_connected'):\n",
    "        name = 'fc-{0}'\n",
    "        if 'weights' in var.name:\n",
    "            name = name.format('weights')\n",
    "        else:\n",
    "            name = name.format('biases')\n",
    "        with tf.variable_scope(name):\n",
    "            formatSummaries(var)  \n",
    "    \n",
    "    for var in tf.trainable_variables('dense'):\n",
    "        print(var.name)\n",
    "        name = var.name.split(\"/\")[0]+\"-{0}\"\n",
    "        if 'kernel' in var.name:\n",
    "            name = name.format('weights')\n",
    "        else:\n",
    "            name = name.format('biases')\n",
    "        with tf.variable_scope(name):\n",
    "            formatSummaries(var)  \n",
    "\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 0/1', 'Iteration: 5', 'Train loss: 0.26995045')\n",
      "('Epoch: 0/1', 'Iteration: 10', 'Train loss: 0.23083532')\n",
      "('Epoch: 0/1', 'Iteration: 15', 'Train loss: 0.38150847')\n",
      "('Epoch: 0/1', 'Iteration: 20', 'Train loss: 0.29914644')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        train_writer = tf.summary.FileWriter(\"./tensorboard-logs/try{0}/train\".format(experimentNumber),\n",
    "                                      sess.graph)\n",
    "        validation_writer = tf.summary.FileWriter(\"./tensorboard-logs/try{0}/val\".format(experimentNumber),\n",
    "                              sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 1\n",
    "        for epoch in range(epochNumber):\n",
    "            for index, (x, y) in enumerate(getBatches(train_x, train_y, batchSize),1):\n",
    "                feed = {\n",
    "                        inputs: x, \n",
    "                        labels:y, \n",
    "                        lr: learningRate,\n",
    "                        drop_keep_porob: keepProbability}\n",
    "                \n",
    "                sess.run([optimiser], feed_dict=feed)\n",
    "\n",
    "                if iteration%5==0:\n",
    "                    summary,loss = sess.run([merged, cost],\n",
    "                        feed_dict=feed)                    \n",
    "                    print(\n",
    "                    \"Epoch: {0}/{1}\".format(epoch, epochNumber),\n",
    "                    \"Iteration: {0}\".format(iteration),\n",
    "                    \"Train loss: {:.8f}\".format(loss))\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                if iteration%25==0:\n",
    "                    val_acc = []\n",
    "                    for x1,y1 in getBatches(val_x, val_y, batchSize):\n",
    "                        feed = {inputs: x1, \n",
    "                                labels: y1,\n",
    "                                lr: learningRate,\n",
    "                                drop_keep_porob:1.0}\n",
    "                        summary,batch_acc = sess.run([merged, accuracy], feed_dict=feed)\n",
    "                        val_acc.append(batch_acc)\n",
    "                    print(\"Val acc: {:.8f}\".format(np.mean(val_acc)))\n",
    "                    validation_writer.add_summary(summary, iteration)\n",
    "\n",
    "                iteration +=1\n",
    "            saver.save(sess, \"checkpoints/model{0}/predictor.ckpt\".format(modelNumber))\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model1/predictor.ckpt\n",
      "Test accuracy: 0.712\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/model{0}'.format(modelNumber)))\n",
    "    for ii, (x, y) in enumerate(getBatches(test_x, test_y, batchSize), 1):\n",
    "        feed = {inputs: x,\n",
    "                labels: y,\n",
    "                drop_keep_porob: 1.}\n",
    "        batch_acc = sess.run([accuracy], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(1-np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
